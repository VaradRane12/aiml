{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a97bd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# --- Cell 1: Load and Split Data ---\n",
    "\n",
    "categories = ['sci.med', 'sci.space']\n",
    "data = fetch_20newsgroups(subset='all', categories=categories, shuffle=True, random_state=42)\n",
    "\n",
    "X = data.data\n",
    "y = data.target\n",
    "target_names = data.target_names\n",
    "\n",
    "X_train_text, X_test_text, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# --- Cell 2: Vectorize Text Data ---\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "X_train = vectorizer.fit_transform(X_train_text).toarray()\n",
    "X_test = vectorizer.transform(X_test_text).toarray()\n",
    "\n",
    "# --- Cell 3: Train Naive Bayes ---\n",
    "\n",
    "alpha = 1.0 # Smoothing parameter\n",
    "n_samples, n_features = X_train.shape\n",
    "classes_ = np.unique(y_train)\n",
    "n_classes = len(classes_)\n",
    "\n",
    "class_log_prior_ = np.zeros(n_classes)\n",
    "feature_log_prob_ = np.zeros((n_classes, n_features))\n",
    "\n",
    "for i, c in enumerate(classes_):\n",
    "    X_c = X_train[y_train == c]\n",
    "    \n",
    "    # Calculate class log prior: log(P(c))\n",
    "    class_log_prior_[i] = np.log(X_c.shape[0] / n_samples)\n",
    "    \n",
    "    total_word_count_in_class = np.sum(X_c)\n",
    "    word_counts_in_class = np.sum(X_c, axis=0)\n",
    "    \n",
    "    # Calculate feature log probability with Laplace smoothing: log(P(w|c))\n",
    "    numerator = word_counts_in_class + alpha\n",
    "    denominator = total_word_count_in_class + alpha * n_features\n",
    "    \n",
    "    feature_log_prob_[i, :] = np.log(numerator / denominator)\n",
    "\n",
    "# --- Cell 4: Predict on Test Data ---\n",
    "\n",
    "# Calculate log probabilities for each class for all test samples\n",
    "# log(P(c|d)) = log(P(c)) + sum_w( log(P(w|c)) * count(w,d) )\n",
    "log_probs = X_test @ feature_log_prob_.T + class_log_prior_\n",
    "\n",
    "# Get the class with the highest log probability for each sample\n",
    "y_pred = classes_[np.argmax(log_probs, axis=1)]\n",
    "\n",
    "# --- Cell 5: Evaluate Performance ---\n",
    "\n",
    "# Calculate Accuracy\n",
    "accuracy = np.mean(y_test == y_pred)\n",
    "print(\"--- Model Evaluation Metrics ---\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Calculate Confusion Matrix\n",
    "cm_classes = np.unique(y_test)\n",
    "n_cm_classes = len(cm_classes)\n",
    "cm = np.zeros((n_cm_classes, n_cm_classes), dtype=int)\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    true_idx = np.where(cm_classes == y_test[i])[0][0]\n",
    "    pred_idx = np.where(cm_classes == y_pred[i])[0][0]\n",
    "    cm[true_idx, pred_idx] += 1\n",
    "\n",
    "print(\"\\n--- Confusion Matrix ---\")\n",
    "print(cm)\n",
    "\n",
    "# Calculate Classification Report\n",
    "precision = np.diag(cm) / np.sum(cm, axis=0)\n",
    "recall = np.diag(cm) / np.sum(cm, axis=1)\n",
    "f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "support = np.sum(cm, axis=1)\n",
    "\n",
    "overall_accuracy = np.sum(np.diag(cm)) / np.sum(cm)\n",
    "\n",
    "avg_precision = np.mean(precision)\n",
    "avg_recall = np.mean(recall)\n",
    "avg_f1 = np.mean(f1_score)\n",
    "\n",
    "weighted_precision = np.sum(precision * support) / np.sum(support)\n",
    "weighted_recall = np.sum(recall * support) / np.sum(support)\n",
    "weighted_f1 = np.sum(f1_score * support) / np.sum(support)\n",
    "\n",
    "# --- Cell 6: Display Classification Report ---\n",
    "\n",
    "report = \"\\n--- Classification Report ---\\n\\n\"\n",
    "report += f\"{'':>10} {'precision':>10} {'recall':>10} {'f1-score':>10} {'support':>10}\\n\\n\"\n",
    "\n",
    "for i, name in enumerate(target_names):\n",
    "    report += f\"{name:>10} {precision[i]:10.4f} {recall[i]:10.4f} {f1_score[i]:10.4f} {support[i]:10}\\n\"\n",
    "    \n",
    "report += \"\\n\"\n",
    "report += f\"{'accuracy':>10} {'':>10} {'':>10} {overall_accuracy:10.4f} {np.sum(support):10}\\n\"\n",
    "report += f\"{'macro avg':>10} {avg_precision:10.4f} {avg_recall:10.4f} {avg_f1:10.4f} {np.sum(support):10}\\n\"\n",
    "report += f\"{'weighted avg':>10} {weighted_precision:10.4f} {weighted_recall:10.4f} {weighted_f1:10.4f} {np.sum(support):10}\\n\"\n",
    "\n",
    "print(report)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4739955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "henllo\n"
     ]
    }
   ],
   "source": [
    "print(\"henllo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24bbd38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
