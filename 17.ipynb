{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1a2487",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# --- Cell 1: Load and Split Data ---\u001b[39;00m\n\u001b[32m      8\u001b[39m categories = [\u001b[33m'\u001b[39m\u001b[33msci.med\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33msci.space\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m data = \u001b[43mfetch_20newsgroups\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubset\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mall\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcategories\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcategories\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m42\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m X = data.data\n\u001b[32m     12\u001b[39m y = data.target\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Asus\\Documents\\Programming\\aiml\\.venv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:218\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    214\u001b[39m         skip_parameter_validation=(\n\u001b[32m    215\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    216\u001b[39m         )\n\u001b[32m    217\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    224\u001b[39m     msg = re.sub(\n\u001b[32m    225\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    226\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    227\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    228\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Asus\\Documents\\Programming\\aiml\\.venv\\Lib\\site-packages\\sklearn\\datasets\\_twenty_newsgroups.py:320\u001b[39m, in \u001b[36mfetch_20newsgroups\u001b[39m\u001b[34m(data_home, subset, categories, shuffle, random_state, remove, download_if_missing, return_X_y, n_retries, delay)\u001b[39m\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m download_if_missing:\n\u001b[32m    319\u001b[39m     logger.info(\u001b[33m\"\u001b[39m\u001b[33mDownloading 20news dataset. This may take a few minutes.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m320\u001b[39m     cache = \u001b[43m_download_20newsgroups\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    321\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtarget_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtwenty_home\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    322\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    323\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_retries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    324\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdelay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdelay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    325\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    326\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    327\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33m20Newsgroups dataset not found\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Asus\\Documents\\Programming\\aiml\\.venv\\Lib\\site-packages\\sklearn\\datasets\\_twenty_newsgroups.py:85\u001b[39m, in \u001b[36m_download_20newsgroups\u001b[39m\u001b[34m(target_dir, cache_path, n_retries, delay)\u001b[39m\n\u001b[32m     83\u001b[39m logger.debug(\u001b[33m\"\u001b[39m\u001b[33mDecompressing \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m, archive_path)\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m tarfile.open(archive_path, \u001b[33m\"\u001b[39m\u001b[33mr:gz\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fp:\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m     \u001b[43mtarfile_extractall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtarget_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m suppress(\u001b[38;5;167;01mFileNotFoundError\u001b[39;00m):\n\u001b[32m     88\u001b[39m     os.remove(archive_path)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Asus\\Documents\\Programming\\aiml\\.venv\\Lib\\site-packages\\sklearn\\utils\\fixes.py:370\u001b[39m, in \u001b[36mtarfile_extractall\u001b[39m\u001b[34m(tarfile, path)\u001b[39m\n\u001b[32m    365\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtarfile_extractall\u001b[39m(tarfile, path):\n\u001b[32m    366\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    367\u001b[39m         \u001b[38;5;66;03m# Use filter=\"data\" to prevent the most dangerous security issues.\u001b[39;00m\n\u001b[32m    368\u001b[39m         \u001b[38;5;66;03m# For more details, see\u001b[39;00m\n\u001b[32m    369\u001b[39m         \u001b[38;5;66;03m# https://docs.python.org/3/library/tarfile.html#tarfile.TarFile.extractall\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m370\u001b[39m         \u001b[43mtarfile\u001b[49m\u001b[43m.\u001b[49m\u001b[43mextractall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    371\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m    372\u001b[39m         tarfile.extractall(path)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\Anaconda\\Lib\\tarfile.py:2352\u001b[39m, in \u001b[36mTarFile.extractall\u001b[39m\u001b[34m(self, path, members, numeric_owner, filter)\u001b[39m\n\u001b[32m   2347\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m tarinfo.isdir():\n\u001b[32m   2348\u001b[39m         \u001b[38;5;66;03m# For directories, delay setting attributes until later,\u001b[39;00m\n\u001b[32m   2349\u001b[39m         \u001b[38;5;66;03m# since permissions can interfere with extraction and\u001b[39;00m\n\u001b[32m   2350\u001b[39m         \u001b[38;5;66;03m# extracting contents can reset mtime.\u001b[39;00m\n\u001b[32m   2351\u001b[39m         directories.append(unfiltered)\n\u001b[32m-> \u001b[39m\u001b[32m2352\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_extract_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mset_attrs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtarinfo\u001b[49m\u001b[43m.\u001b[49m\u001b[43misdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2353\u001b[39m \u001b[43m                      \u001b[49m\u001b[43mnumeric_owner\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnumeric_owner\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2354\u001b[39m \u001b[43m                      \u001b[49m\u001b[43mfilter_function\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilter_function\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2356\u001b[39m \u001b[38;5;66;03m# Reverse sort directories.\u001b[39;00m\n\u001b[32m   2357\u001b[39m directories.sort(key=\u001b[38;5;28;01mlambda\u001b[39;00m a: a.name, reverse=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\Anaconda\\Lib\\tarfile.py:2455\u001b[39m, in \u001b[36mTarFile._extract_one\u001b[39m\u001b[34m(self, tarinfo, path, set_attrs, numeric_owner, filter_function)\u001b[39m\n\u001b[32m   2452\u001b[39m \u001b[38;5;28mself\u001b[39m._check(\u001b[33m\"\u001b[39m\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2454\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2455\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_extract_member\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarinfo\u001b[49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2456\u001b[39m \u001b[43m                         \u001b[49m\u001b[43mset_attrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mset_attrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2457\u001b[39m \u001b[43m                         \u001b[49m\u001b[43mnumeric_owner\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnumeric_owner\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2458\u001b[39m \u001b[43m                         \u001b[49m\u001b[43mfilter_function\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilter_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2459\u001b[39m \u001b[43m                         \u001b[49m\u001b[43mextraction_root\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2460\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mOSError\u001b[39;00m, \u001b[38;5;167;01mUnicodeEncodeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   2461\u001b[39m     \u001b[38;5;28mself\u001b[39m._handle_fatal_error(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\Anaconda\\Lib\\tarfile.py:2544\u001b[39m, in \u001b[36mTarFile._extract_member\u001b[39m\u001b[34m(self, tarinfo, targetpath, set_attrs, numeric_owner, filter_function, extraction_root)\u001b[39m\n\u001b[32m   2541\u001b[39m     \u001b[38;5;28mself\u001b[39m._dbg(\u001b[32m1\u001b[39m, tarinfo.name)\n\u001b[32m   2543\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tarinfo.isreg():\n\u001b[32m-> \u001b[39m\u001b[32m2544\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmakefile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargetpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2545\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m tarinfo.isdir():\n\u001b[32m   2546\u001b[39m     \u001b[38;5;28mself\u001b[39m.makedir(tarinfo, targetpath)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\Anaconda\\Lib\\tarfile.py:2593\u001b[39m, in \u001b[36mTarFile.makefile\u001b[39m\u001b[34m(self, tarinfo, targetpath)\u001b[39m\n\u001b[32m   2591\u001b[39m source.seek(tarinfo.offset_data)\n\u001b[32m   2592\u001b[39m bufsize = \u001b[38;5;28mself\u001b[39m.copybufsize\n\u001b[32m-> \u001b[39m\u001b[32m2593\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbltn_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtargetpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mwb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2594\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtarinfo\u001b[49m\u001b[43m.\u001b[49m\u001b[43msparse\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[32m   2595\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtarinfo\u001b[49m\u001b[43m.\u001b[49m\u001b[43msparse\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "#ASS17. Implement the Naïve Bayes algorithm from scratch to solve a real-world classification problem\n",
    "#such as email spam detection, sentiment analysis, or disease diagnosis.\n",
    "\n",
    "# ==========================================================\n",
    "# Email Spam Detection using Naïve Bayes (Fully From Scratch)\n",
    "# ==========================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 1. Load and Prepare Data\n",
    "# ----------------------------------------------------------\n",
    "df = pd.read_csv(\"email.csv\")\n",
    "X = df.drop(columns=[\"Prediction\", \"Email No.\"])\n",
    "y = df[\"Prediction\"]\n",
    "\n",
    "data = pd.concat([X, y], axis=1).dropna()\n",
    "X = data.drop(columns=[\"Prediction\"]).values\n",
    "y = data[\"Prediction\"].values\n",
    "\n",
    "np.random.seed(42)\n",
    "indices = np.arange(len(X))\n",
    "np.random.shuffle(indices)\n",
    "split = int(0.7 * len(X))\n",
    "X_train, X_test = X[indices[:split]], X[indices[split:]]\n",
    "y_train, y_test = y[indices[:split]], y[indices[split:]]\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 2. Naïve Bayes Implementation (From Scratch)\n",
    "# ----------------------------------------------------------\n",
    "class NaiveBayesScratch:\n",
    "    def fit(self, X, y):\n",
    "        self.classes = np.unique(y)\n",
    "        self.class_priors = {c: np.mean(y == c) for c in self.classes}\n",
    "        self.mean = {}\n",
    "        self.var = {}\n",
    "        for c in self.classes:\n",
    "            X_c = X[y == c]\n",
    "            self.mean[c] = X_c.mean(axis=0)\n",
    "            self.var[c] = X_c.var(axis=0) + 1e-9  # avoid zero variance\n",
    "\n",
    "    def _pdf(self, class_idx, x):\n",
    "        mean = self.mean[class_idx]\n",
    "        var = self.var[class_idx]\n",
    "        numerator = np.exp(-((x - mean) ** 2) / (2 * var))\n",
    "        denominator = np.sqrt(2 * np.pi * var)\n",
    "        return numerator / denominator\n",
    "\n",
    "    def _predict_single(self, x):\n",
    "        eps = 1e-9\n",
    "        posteriors = []\n",
    "        for c in self.classes:\n",
    "            prior = np.log(self.class_priors[c] + eps)\n",
    "            probs = self._pdf(c, x)\n",
    "            probs = np.clip(probs, eps, None)\n",
    "            likelihood = np.sum(np.log(probs))\n",
    "            posteriors.append(prior + likelihood)\n",
    "        return self.classes[np.argmax(posteriors)]\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self._predict_single(x) for x in X])\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 3. Train and Predict\n",
    "# ----------------------------------------------------------\n",
    "nb_model = NaiveBayesScratch()\n",
    "nb_model.fit(X_train, y_train)\n",
    "y_pred = nb_model.predict(X_test)\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 4. Evaluate Manually\n",
    "# ----------------------------------------------------------\n",
    "classes = np.unique(y_test)\n",
    "label_to_index = {label: i for i, label in enumerate(classes)}\n",
    "cm = np.zeros((len(classes), len(classes)), dtype=int)\n",
    "\n",
    "for actual, pred in zip(y_test, y_pred):\n",
    "    cm[label_to_index[actual]][label_to_index[pred]] += 1\n",
    "\n",
    "tp = cm[1, 1] if len(classes) == 2 else np.diag(cm)\n",
    "fp = cm.sum(axis=0) - np.diag(cm)\n",
    "fn = cm.sum(axis=1) - np.diag(cm)\n",
    "\n",
    "accuracy = np.trace(cm) / np.sum(cm)\n",
    "precision = np.mean(tp / (tp + fp + 1e-9))\n",
    "recall = np.mean(tp / (tp + fn + 1e-9))\n",
    "f1 = 2 * precision * recall / (precision + recall + 1e-9)\n",
    "\n",
    "print(\"\\nConfusion Matrix (rows=actual, cols=predicted):\")\n",
    "print(classes)\n",
    "print(cm)\n",
    "print(\"\\nEvaluation Metrics:\")\n",
    "print(f\"Accuracy : {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall   : {recall:.4f}\")\n",
    "print(f\"F1-Score : {f1:.4f}\")\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 5. Visualize Confusion Matrix\n",
    "# ----------------------------------------------------------\n",
    "plt.figure(figsize=(5,4))\n",
    "plt.imshow(cm, cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix - Naïve Bayes Spam Classifier (From Scratch)\")\n",
    "plt.xticks(np.arange(len(classes)), classes)\n",
    "plt.yticks(np.arange(len(classes)), classes)\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"Actual Label\")\n",
    "\n",
    "for i in range(len(classes)):\n",
    "    for j in range(len(classes)):\n",
    "        plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > cm.max()/2 else \"black\")\n",
    "\n",
    "plt.colorbar()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ==============================================================\n",
    "# Naïve Bayes Sentiment Analysis from Scratch + Visualization\n",
    "# Dataset: sentinel.csv (columns: text, sentiment)\n",
    "# ==============================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 1. Load Dataset\n",
    "# --------------------------------------------------------------\n",
    "df = pd.read_csv(\"sentiment.csv\", usecols=['text', 'sentiment'])\n",
    "df.dropna(inplace=True)\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "print(\"Dataset Loaded Successfully! Shape:\", df.shape)\n",
    "print(\"Unique Sentiments:\", df['sentiment'].unique())\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 2. Preprocess Function\n",
    "# --------------------------------------------------------------\n",
    "def preprocess(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    return text.split()\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 3. Manual Train-Test Split (70–30)\n",
    "# --------------------------------------------------------------\n",
    "np.random.seed(42)\n",
    "indices = np.random.permutation(len(df))\n",
    "split = int(0.7 * len(df))\n",
    "train_idx, test_idx = indices[:split], indices[split:]\n",
    "\n",
    "train_data = df.iloc[train_idx]\n",
    "test_data = df.iloc[test_idx]\n",
    "\n",
    "print(f\"\\nTrain size: {len(train_data)}, Test size: {len(test_data)}\")\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 4. Train Naïve Bayes Model (from scratch)\n",
    "# --------------------------------------------------------------\n",
    "def train_naive_bayes(data):\n",
    "    class_word_counts = defaultdict(Counter)\n",
    "    class_counts = Counter()\n",
    "    vocab = set()\n",
    "\n",
    "    for _, row in data.iterrows():\n",
    "        label = row['sentiment']\n",
    "        words = preprocess(row['text'])\n",
    "        class_counts[label] += 1\n",
    "        class_word_counts[label].update(words)\n",
    "        vocab.update(words)\n",
    "\n",
    "    total_docs = sum(class_counts.values())\n",
    "    priors = {label: class_counts[label] / total_docs for label in class_counts}\n",
    "    total_words = {label: sum(class_word_counts[label].values()) for label in class_counts}\n",
    "\n",
    "    return vocab, class_word_counts, total_words, priors\n",
    "\n",
    "vocab, class_word_counts, total_words, priors = train_naive_bayes(train_data)\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 5. Likelihood with Laplace Smoothing\n",
    "# --------------------------------------------------------------\n",
    "def word_likelihood(word, label):\n",
    "    return (class_word_counts[label][word] + 1) / (total_words[label] + vocab_size)\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 6. Prediction Function\n",
    "# --------------------------------------------------------------\n",
    "def predict(text):\n",
    "    words = preprocess(text)\n",
    "    scores = {}\n",
    "    for label in priors:\n",
    "        score = math.log(priors[label])\n",
    "        for word in words:\n",
    "            if word in vocab:\n",
    "                score += math.log(word_likelihood(word, label))\n",
    "        scores[label] = score\n",
    "    return max(scores, key=scores.get)\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 7. Evaluate Model\n",
    "# --------------------------------------------------------------\n",
    "y_true = test_data['sentiment'].tolist()\n",
    "y_pred = [predict(t) for t in test_data['text']]\n",
    "\n",
    "labels = sorted(list(set(y_true)))\n",
    "label_to_index = {label: i for i, label in enumerate(labels)}\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = np.zeros((len(labels), len(labels)), dtype=int)\n",
    "for actual, pred in zip(y_true, y_pred):\n",
    "    cm[label_to_index[actual]][label_to_index[pred]] += 1\n",
    "\n",
    "# Accuracy / Precision / Recall / F1\n",
    "correct = sum(a == b for a, b in zip(y_true, y_pred))\n",
    "accuracy = correct / len(y_true)\n",
    "\n",
    "precision, recall, f1 = {}, {}, {}\n",
    "for label in labels:\n",
    "    i = label_to_index[label]\n",
    "    tp = cm[i, i]\n",
    "    fp = sum(cm[:, i]) - tp\n",
    "    fn = sum(cm[i, :]) - tp\n",
    "    precision[label] = tp / (tp + fp + 1e-9)\n",
    "    recall[label] = tp / (tp + fn + 1e-9)\n",
    "    f1[label] = 2 * precision[label] * recall[label] / (precision[label] + recall[label] + 1e-9)\n",
    "\n",
    "print(\"\\nConfusion Matrix (rows=actual, cols=predicted):\")\n",
    "print(labels)\n",
    "print(cm)\n",
    "print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
    "for label in labels:\n",
    "    print(f\"{label.capitalize()} → Precision: {precision[label]:.4f}, Recall: {recall[label]:.4f}, F1: {f1[label]:.4f}\")\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 8. Visualization\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "# (A) Confusion Matrix Heatmap\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.imshow(cm, cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix — Naïve Bayes (From Scratch)\")\n",
    "plt.xticks(np.arange(len(labels)), labels)\n",
    "plt.yticks(np.arange(len(labels)), labels)\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"Actual Label\")\n",
    "\n",
    "# Annotate values\n",
    "for i in range(len(labels)):\n",
    "    for j in range(len(labels)):\n",
    "        plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > cm.max()/2 else \"black\")\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "# (B) Class Distribution in Training Data\n",
    "train_counts = train_data['sentiment'].value_counts()\n",
    "plt.figure(figsize=(5,4))\n",
    "train_counts.plot(kind='bar', color=['skyblue','salmon','lightgreen'])\n",
    "plt.title(\"Training Data Sentiment Distribution\")\n",
    "plt.xlabel(\"Sentiment Class\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# Disease Diagnosis using Naïve Bayes (Implemented From Scratch)\n",
    "# ==========================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 1. Load Dataset\n",
    "# ----------------------------------------------------------\n",
    "df = pd.read_csv(\"disease.csv\")\n",
    "print(\"Dataset Loaded Successfully! Shape:\", df.shape)\n",
    "print(\"\\nFirst 5 Rows:\\n\", df.head())\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 2. Data Preprocessing\n",
    "# ----------------------------------------------------------\n",
    "# Split Blood Pressure into Systolic/Diastolic\n",
    "bp_split = df[\"Blood_Pressure_mmHg\"].str.split(\"/\", expand=True)\n",
    "df[\"BP_Systolic\"] = pd.to_numeric(bp_split[0], errors='coerce')\n",
    "df[\"BP_Diastolic\"] = pd.to_numeric(bp_split[1], errors='coerce')\n",
    "\n",
    "# Drop irrelevant columns\n",
    "df = df.drop(columns=[\"Patient_ID\", \"Blood_Pressure_mmHg\", \"Treatment_Plan\"])\n",
    "\n",
    "# Encode all categorical columns automatically\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == \"object\":\n",
    "        df[col] = df[col].astype(\"category\").cat.codes\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop(columns=[\"Diagnosis\"]).values\n",
    "y = df[\"Diagnosis\"].values\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 3. Manual Train/Test Split (70/30)\n",
    "# ----------------------------------------------------------\n",
    "np.random.seed(42)\n",
    "indices = np.arange(len(X))\n",
    "np.random.shuffle(indices)\n",
    "split = int(0.7 * len(X))\n",
    "X_train, X_test = X[indices[:split]], X[indices[split:]]\n",
    "y_train, y_test = y[indices[:split]], y[indices[split:]]\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 4. Naïve Bayes Implementation (From Scratch)\n",
    "# ----------------------------------------------------------\n",
    "class NaiveBayesScratch:\n",
    "    def fit(self, X, y):\n",
    "        self.classes = np.unique(y)\n",
    "        self.class_priors = {c: np.mean(y == c) for c in self.classes}\n",
    "        self.mean = {}\n",
    "        self.var = {}\n",
    "        for c in self.classes:\n",
    "            X_c = X[y == c]\n",
    "            self.mean[c] = X_c.mean(axis=0)\n",
    "            self.var[c] = X_c.var(axis=0) + 1e-9  # avoid zero variance\n",
    "\n",
    "    def _pdf(self, class_idx, x):\n",
    "        mean = self.mean[class_idx]\n",
    "        var = self.var[class_idx]\n",
    "        numerator = np.exp(-((x - mean) ** 2) / (2 * var))\n",
    "        denominator = np.sqrt(2 * np.pi * var)\n",
    "        return numerator / denominator\n",
    "\n",
    "    def _predict_single(self, x):\n",
    "        eps = 1e-9\n",
    "        posteriors = []\n",
    "        for c in self.classes:\n",
    "            prior = np.log(self.class_priors[c] + eps)\n",
    "            probs = np.clip(self._pdf(c, x), eps, None)\n",
    "            likelihood = np.sum(np.log(probs))\n",
    "            posteriors.append(prior + likelihood)\n",
    "        return self.classes[np.argmax(posteriors)]\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self._predict_single(x) for x in X])\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 5. Train and Predict\n",
    "# ----------------------------------------------------------\n",
    "nb_model = NaiveBayesScratch()\n",
    "nb_model.fit(X_train, y_train)\n",
    "y_pred = nb_model.predict(X_test)\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 6. Evaluate Manually\n",
    "# ----------------------------------------------------------\n",
    "classes = np.unique(y_test)\n",
    "label_to_index = {label: i for i, label in enumerate(classes)}\n",
    "cm = np.zeros((len(classes), len(classes)), dtype=int)\n",
    "\n",
    "for actual, pred in zip(y_test, y_pred):\n",
    "    cm[label_to_index[actual]][label_to_index[pred]] += 1\n",
    "\n",
    "tp = np.diag(cm)\n",
    "fp = cm.sum(axis=0) - np.diag(cm)\n",
    "fn = cm.sum(axis=1) - np.diag(cm)\n",
    "\n",
    "accuracy = np.trace(cm) / np.sum(cm)\n",
    "precision = np.mean(tp / (tp + fp + 1e-9))\n",
    "recall = np.mean(tp / (tp + fn + 1e-9))\n",
    "f1 = 2 * precision * recall / (precision + recall + 1e-9)\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "print(\"\\nEvaluation Metrics:\")\n",
    "print(f\"Accuracy : {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall   : {recall:.4f}\")\n",
    "print(f\"F1-Score : {f1:.4f}\")\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 7. Visualize Confusion Matrix\n",
    "# ----------------------------------------------------------\n",
    "plt.figure(figsize=(5,4))\n",
    "plt.imshow(cm, cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix - Naïve Bayes Disease Diagnosis (From Scratch)\")\n",
    "plt.xticks(np.arange(len(classes)), classes)\n",
    "plt.yticks(np.arange(len(classes)), classes)\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"Actual Label\")\n",
    "\n",
    "for i in range(len(classes)):\n",
    "    for j in range(len(classes)):\n",
    "        plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > cm.max()/2 else \"black\")\n",
    "\n",
    "plt.colorbar()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1886f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
