{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5ZhrGJmPB_s"
      },
      "source": [
        "17"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pawezjXwqka4",
        "outputId": "ed55b89f-c95c-4962-e846-0c0cc9d01e83"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Model Evaluation Metrics ---\n",
            "Accuracy: 0.9848\n",
            "\n",
            "--- Confusion Matrix ---\n",
            "[[286   6]\n",
            " [  3 299]]\n",
            "\n",
            "--- Classification Report ---\n",
            "\n",
            "            precision     recall   f1-score    support\n",
            "\n",
            "   sci.med     0.9896     0.9795     0.9845        292\n",
            " sci.space     0.9803     0.9901     0.9852        302\n",
            "\n",
            "  accuracy                           0.9848        594\n",
            " macro avg     0.9850     0.9848     0.9848        594\n",
            "weighted avg     0.9849     0.9848     0.9848        594\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# --- Cell 1: Load and Split Data ---\n",
        "\n",
        "categories = ['sci.med', 'sci.space']\n",
        "data = fetch_20newsgroups(subset='all', categories=categories, shuffle=True, random_state=42)\n",
        "\n",
        "X = data.data\n",
        "y = data.target\n",
        "target_names = data.target_names\n",
        "\n",
        "X_train_text, X_test_text, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# --- Cell 2: Vectorize Text Data ---\n",
        "\n",
        "vectorizer = CountVectorizer(stop_words='english')\n",
        "X_train = vectorizer.fit_transform(X_train_text).toarray()\n",
        "X_test = vectorizer.transform(X_test_text).toarray()\n",
        "\n",
        "# --- Cell 3: Train Naive Bayes ---\n",
        "\n",
        "alpha = 1.0 # Smoothing parameter\n",
        "n_samples, n_features = X_train.shape\n",
        "classes_ = np.unique(y_train)\n",
        "n_classes = len(classes_)\n",
        "\n",
        "class_log_prior_ = np.zeros(n_classes)\n",
        "feature_log_prob_ = np.zeros((n_classes, n_features))\n",
        "\n",
        "for i, c in enumerate(classes_):\n",
        "    X_c = X_train[y_train == c]\n",
        "\n",
        "    # Calculate class log prior: log(P(c))\n",
        "    class_log_prior_[i] = np.log(X_c.shape[0] / n_samples)\n",
        "\n",
        "    total_word_count_in_class = np.sum(X_c)\n",
        "    word_counts_in_class = np.sum(X_c, axis=0)\n",
        "\n",
        "    # Calculate feature log probability with Laplace smoothing: log(P(w|c))\n",
        "    numerator = word_counts_in_class + alpha\n",
        "    denominator = total_word_count_in_class + alpha * n_features\n",
        "\n",
        "    feature_log_prob_[i, :] = np.log(numerator / denominator)\n",
        "\n",
        "# --- Cell 4: Predict on Test Data ---\n",
        "\n",
        "# Calculate log probabilities for each class for all test samples\n",
        "# log(P(c|d)) = log(P(c)) + sum_w( log(P(w|c)) * count(w,d) )\n",
        "log_probs = X_test @ feature_log_prob_.T + class_log_prior_\n",
        "\n",
        "# Get the class with the highest log probability for each sample\n",
        "y_pred = classes_[np.argmax(log_probs, axis=1)]\n",
        "\n",
        "# --- Cell 5: Evaluate Performance ---\n",
        "\n",
        "# Calculate Accuracy\n",
        "accuracy = np.mean(y_test == y_pred)\n",
        "print(\"--- Model Evaluation Metrics ---\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Calculate Confusion Matrix\n",
        "cm_classes = np.unique(y_test)\n",
        "n_cm_classes = len(cm_classes)\n",
        "cm = np.zeros((n_cm_classes, n_cm_classes), dtype=int)\n",
        "\n",
        "for i in range(len(y_test)):\n",
        "    true_idx = np.where(cm_classes == y_test[i])[0][0]\n",
        "    pred_idx = np.where(cm_classes == y_pred[i])[0][0]\n",
        "    cm[true_idx, pred_idx] += 1\n",
        "\n",
        "print(\"\\n--- Confusion Matrix ---\")\n",
        "print(cm)\n",
        "\n",
        "# Calculate Classification Report\n",
        "precision = np.diag(cm) / np.sum(cm, axis=0)\n",
        "recall = np.diag(cm) / np.sum(cm, axis=1)\n",
        "f1_score = 2 * (precision * recall) / (precision + recall)\n",
        "support = np.sum(cm, axis=1)\n",
        "\n",
        "overall_accuracy = np.sum(np.diag(cm)) / np.sum(cm)\n",
        "\n",
        "avg_precision = np.mean(precision)\n",
        "avg_recall = np.mean(recall)\n",
        "avg_f1 = np.mean(f1_score)\n",
        "\n",
        "weighted_precision = np.sum(precision * support) / np.sum(support)\n",
        "weighted_recall = np.sum(recall * support) / np.sum(support)\n",
        "weighted_f1 = np.sum(f1_score * support) / np.sum(support)\n",
        "\n",
        "# --- Cell 6: Display Classification Report ---\n",
        "\n",
        "report = \"\\n--- Classification Report ---\\n\\n\"\n",
        "report += f\"{'':>10} {'precision':>10} {'recall':>10} {'f1-score':>10} {'support':>10}\\n\\n\"\n",
        "\n",
        "for i, name in enumerate(target_names):\n",
        "    report += f\"{name:>10} {precision[i]:10.4f} {recall[i]:10.4f} {f1_score[i]:10.4f} {support[i]:10}\\n\"\n",
        "\n",
        "report += \"\\n\"\n",
        "report += f\"{'accuracy':>10} {'':>10} {'':>10} {overall_accuracy:10.4f} {np.sum(support):10}\\n\"\n",
        "report += f\"{'macro avg':>10} {avg_precision:10.4f} {avg_recall:10.4f} {avg_f1:10.4f} {np.sum(support):10}\\n\"\n",
        "report += f\"{'weighted avg':>10} {weighted_precision:10.4f} {weighted_recall:10.4f} {weighted_f1:10.4f} {np.sum(support):10}\\n\"\n",
        "\n",
        "print(report)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBChZVo9O-Wj"
      },
      "source": [
        "18"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rvPgMCNLqmCT",
        "outputId": "d837751f-24ba-4f99-e922-cf8abd8e79ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Model Evaluation Metrics ---\n",
            "Accuracy: 0.9848\n",
            "\n",
            "--- Confusion Matrix ---\n",
            "[[286   6]\n",
            " [  3 299]]\n",
            "\n",
            "--- Classification Report ---\n",
            "\n",
            "            precision     recall   f1-score    support\n",
            "\n",
            "   sci.med     0.9896     0.9795     0.9845        292\n",
            " sci.space     0.9803     0.9901     0.9852        302\n",
            "\n",
            "  accuracy                           0.9848        594\n",
            " macro avg     0.9850     0.9848     0.9848        594\n",
            "weighted avg     0.9849     0.9848     0.9848        594\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# --- Cell 1: Load and Split Data ---\n",
        "\n",
        "categories = ['sci.med', 'sci.space']\n",
        "data = fetch_20newsgroups(subset='all', categories=categories, shuffle=True, random_state=42)\n",
        "\n",
        "X = data.data\n",
        "y = data.target\n",
        "target_names = data.target_names\n",
        "\n",
        "X_train_text, X_test_text, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# --- Cell 2: Vectorize Text Data ---\n",
        "\n",
        "vectorizer = CountVectorizer(stop_words='english')\n",
        "X_train = vectorizer.fit_transform(X_train_text).toarray()\n",
        "X_test = vectorizer.transform(X_test_text).toarray()\n",
        "\n",
        "# --- Cell 3: Train Naive Bayes ---\n",
        "\n",
        "alpha = 1.0 # Smoothing parameter\n",
        "n_samples, n_features = X_train.shape\n",
        "classes_ = np.unique(y_train)\n",
        "n_classes = len(classes_)\n",
        "\n",
        "class_log_prior_ = np.zeros(n_classes)\n",
        "feature_log_prob_ = np.zeros((n_classes, n_features))\n",
        "\n",
        "for i, c in enumerate(classes_):\n",
        "    X_c = X_train[y_train == c]\n",
        "\n",
        "    # Calculate class log prior: log(P(c))\n",
        "    class_log_prior_[i] = np.log(X_c.shape[0] / n_samples)\n",
        "\n",
        "    total_word_count_in_class = np.sum(X_c)\n",
        "    word_counts_in_class = np.sum(X_c, axis=0)\n",
        "\n",
        "    # Calculate feature log probability with Laplace smoothing: log(P(w|c))\n",
        "    numerator = word_counts_in_class + alpha\n",
        "    denominator = total_word_count_in_class + alpha * n_features\n",
        "\n",
        "    feature_log_prob_[i, :] = np.log(numerator / denominator)\n",
        "\n",
        "# --- Cell 4: Predict on Test Data ---\n",
        "\n",
        "# Calculate log probabilities for each class for all test samples\n",
        "# log(P(c|d)) = log(P(c)) + sum_w( log(P(w|c)) * count(w,d) )\n",
        "log_probs = X_test @ feature_log_prob_.T + class_log_prior_\n",
        "\n",
        "# Get the class with the highest log probability for each sample\n",
        "y_pred = classes_[np.argmax(log_probs, axis=1)]\n",
        "\n",
        "# --- Cell 5: Evaluate Performance ---\n",
        "\n",
        "# Calculate Accuracy\n",
        "accuracy = np.mean(y_test == y_pred)\n",
        "print(\"--- Model Evaluation Metrics ---\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Calculate Confusion Matrix\n",
        "cm_classes = np.unique(y_test)\n",
        "n_cm_classes = len(cm_classes)\n",
        "cm = np.zeros((n_cm_classes, n_cm_classes), dtype=int)\n",
        "\n",
        "for i in range(len(y_test)):\n",
        "    true_idx = np.where(cm_classes == y_test[i])[0][0]\n",
        "    pred_idx = np.where(cm_classes == y_pred[i])[0][0]\n",
        "    cm[true_idx, pred_idx] += 1\n",
        "\n",
        "print(\"\\n--- Confusion Matrix ---\")\n",
        "print(cm)\n",
        "\n",
        "# Calculate Classification Report\n",
        "precision = np.diag(cm) / np.sum(cm, axis=0)\n",
        "recall = np.diag(cm) / np.sum(cm, axis=1)\n",
        "f1_score = 2 * (precision * recall) / (precision + recall)\n",
        "support = np.sum(cm, axis=1)\n",
        "\n",
        "overall_accuracy = np.sum(np.diag(cm)) / np.sum(cm)\n",
        "\n",
        "avg_precision = np.mean(precision)\n",
        "avg_recall = np.mean(recall)\n",
        "avg_f1 = np.mean(f1_score)\n",
        "\n",
        "weighted_precision = np.sum(precision * support) / np.sum(support)\n",
        "weighted_recall = np.sum(recall * support) / np.sum(support)\n",
        "weighted_f1 = np.sum(f1_score * support) / np.sum(support)\n",
        "\n",
        "# --- Cell 6: Display Classification Report ---\n",
        "\n",
        "report = \"\\n--- Classification Report ---\\n\\n\"\n",
        "report += f\"{'':>10} {'precision':>10} {'recall':>10} {'f1-score':>10} {'support':>10}\\n\\n\"\n",
        "\n",
        "for i, name in enumerate(target_names):\n",
        "    report += f\"{name:>10} {precision[i]:10.4f} {recall[i]:10.4f} {f1_score[i]:10.4f} {support[i]:10}\\n\"\n",
        "\n",
        "report += \"\\n\"\n",
        "report += f\"{'accuracy':>10} {'':>10} {'':>10} {overall_accuracy:10.4f} {np.sum(support):10}\\n\"\n",
        "report += f\"{'macro avg':>10} {avg_precision:10.4f} {avg_recall:10.4f} {avg_f1:10.4f} {np.sum(support):10}\\n\"\n",
        "report += f\"{'weighted avg':>10} {weighted_precision:10.4f} {weighted_recall:10.4f} {weighted_f1:10.4f} {np.sum(support):10}\\n\"\n",
        "\n",
        "print(report)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qCiHpAfO6Qc"
      },
      "source": [
        "19"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1K31MnJxq4o5",
        "outputId": "dac3d396-85b1-4882-ed50-1c5ea9f625a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Model Evaluation Results (SVM From Scratch) ---\n",
            "Accuracy: 1.0000\n",
            "\n",
            "--- Confusion Matrix ---\n",
            "[[90  0]\n",
            " [ 0 60]]\n",
            "\n",
            "--- Classification Report ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "Not Spam (0)       1.00      1.00      1.00        90\n",
            "    Spam (1)       1.00      1.00      1.00        60\n",
            "\n",
            "    accuracy                           1.00       150\n",
            "   macro avg       1.00      1.00      1.00       150\n",
            "weighted avg       1.00      1.00      1.00       150\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "# --- Cell 1: Generate Synthetic Data ---\n",
        "\n",
        "np.random.seed(42)\n",
        "data_size = 500\n",
        "spam_count = int(data_size * 0.4) # More balanced for this example\n",
        "not_spam_count = data_size - spam_count\n",
        "\n",
        "features_not_spam = pd.DataFrame({\n",
        "    'word_count': np.random.normal(150, 40, not_spam_count),\n",
        "    'special_chars': np.random.normal(10, 5, not_spam_count),\n",
        "    'capitals_ratio': np.random.uniform(0.01, 0.1, not_spam_count)\n",
        "})\n",
        "labels_not_spam = pd.Series(np.zeros(not_spam_count, dtype=int))\n",
        "\n",
        "features_spam = pd.DataFrame({\n",
        "    'word_count': np.random.normal(250, 50, spam_count),\n",
        "    'special_chars': np.random.normal(30, 10, spam_count),\n",
        "    'capitals_ratio': np.random.uniform(0.2, 0.5, spam_count)\n",
        "})\n",
        "labels_spam = pd.Series(np.ones(spam_count, dtype=int))\n",
        "\n",
        "X = pd.concat([features_not_spam, features_spam]).reset_index(drop=True)\n",
        "y = pd.concat([labels_not_spam, labels_spam]).reset_index(drop=True)\n",
        "\n",
        "X[X < 0] = 0\n",
        "\n",
        "# --- Cell 2: Preprocess Data ---\n",
        "\n",
        "# Map labels from {0, 1} to {-1, 1} for SVM\n",
        "y_svm = y.map({0: -1, 1: 1}).values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_svm, test_size=0.3, random_state=42, stratify=y_svm)\n",
        "\n",
        "# Scale data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# --- Cell 3: SVM \"from scratch\" Implementation (Sequential) ---\n",
        "\n",
        "# --- Hyperparameters ---\n",
        "learning_rate = 0.001\n",
        "lambda_param = 0.01 # Regularization parameter (C = 1/lambda_param)\n",
        "n_iters = 1000\n",
        "\n",
        "# --- Initialization ---\n",
        "n_samples, n_features = X_train_scaled.shape\n",
        "w = np.zeros(n_features) # Weights\n",
        "b = 0 # Bias\n",
        "\n",
        "# --- Gradient Descent Training Loop ---\n",
        "for _ in range(n_iters):\n",
        "\n",
        "    # Iterate over each training sample\n",
        "    for idx, x_i in enumerate(X_train_scaled):\n",
        "\n",
        "        # Calculate condition: y_i * (w . x_i + b) >= 1\n",
        "        condition = y_train[idx] * (np.dot(x_i, w) + b)\n",
        "\n",
        "        # Hinge Loss Gradient\n",
        "        if condition >= 1:\n",
        "            # We are in the correct margin\n",
        "            # Gradient of regularization term: 2 * lambda * w\n",
        "            dw = 2 * lambda_param * w\n",
        "            db = 0\n",
        "        else:\n",
        "            # We are inside the margin or on the wrong side\n",
        "            # Gradient of loss: 2 * lambda * w - y_i * x_i\n",
        "            # Gradient of bias: -y_i\n",
        "            dw = 2 * lambda_param * w - y_train[idx] * x_i\n",
        "            db = -y_train[idx]\n",
        "\n",
        "        # Update weights and bias\n",
        "        w = w - learning_rate * dw\n",
        "        b = b - learning_rate * db\n",
        "\n",
        "# --- Cell 4: Predict ---\n",
        "\n",
        "# Calculate the linear output: w . x + b\n",
        "linear_output = np.dot(X_test_scaled, w) + b\n",
        "\n",
        "# Predictions are the sign of the output\n",
        "# np.sign returns -1 or 1, which matches our y_test labels\n",
        "y_pred = np.sign(linear_output)\n",
        "\n",
        "# --- Cell 5: Evaluate Performance ---\n",
        "\n",
        "# Map labels back to {0, 1} for metrics\n",
        "y_test_mapped = (y_test + 1) // 2 # {-1 -> 0, 1 -> 1}\n",
        "y_pred_mapped = (y_pred + 1) // 2 # {-1 -> 0, 1 -> 1}\n",
        "\n",
        "# --- Cell 6: Display Results ---\n",
        "\n",
        "accuracy = accuracy_score(y_test_mapped, y_pred_mapped)\n",
        "cm = confusion_matrix(y_test_mapped, y_pred_mapped)\n",
        "report = classification_report(y_test_mapped, y_pred_mapped, target_names=['Not Spam (0)', 'Spam (1)'])\n",
        "\n",
        "print(\"--- Model Evaluation Results (SVM From Scratch) ---\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(\"\\n--- Confusion Matrix ---\")\n",
        "print(cm)\n",
        "print(\"\\n--- Classification Report ---\")\n",
        "print(report)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RG1zKpB6O108"
      },
      "source": [
        "20\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JKCPBWZMq-uf",
        "outputId": "bd46ce78-1eaa-458a-be83-8cd866a1ba18"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Model Evaluation Results (SVM From Scratch w/ Poly Features) ---\n",
            "Accuracy: 0.8867\n",
            "\n",
            "--- Confusion Matrix ---\n",
            "[[92  7]\n",
            " [10 41]]\n",
            "\n",
            "--- Performance Metrics ---\n",
            "           precision  recall     f1-score  \n",
            "----------------------------------------\n",
            "Fail (0)   0.9020     0.9293     0.9154    \n",
            "Pass (1)   0.8542     0.8039     0.8283    \n",
            "\n",
            "--- Key Metrics (for 'Pass' class) ---\n",
            "Precision: 0.8542\n",
            "Recall:    0.8039\n",
            "F1-Score:  0.8283\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# --- Cell 1: Generate Synthetic Data ---\n",
        "# Create a synthetic dataset for student performance\n",
        "np.random.seed(42)\n",
        "data_size = 500\n",
        "\n",
        "# Features: study time (hours/week), absences (days), internal scores (0-50)\n",
        "study_time = np.random.uniform(1, 20, data_size)\n",
        "absences = np.random.randint(0, 30, data_size)\n",
        "internal_scores = np.random.uniform(0, 50, data_size)\n",
        "\n",
        "# Target: Pass (1) or Fail (0)\n",
        "# Create a non-linear relationship:\n",
        "# High internal scores + high study time = Pass\n",
        "# High absences = Fail\n",
        "# (internal_scores/50 + study_time/20) - (absences/30) + noise\n",
        "probability = (internal_scores/50 + study_time/20) - (absences/30) + np.random.normal(0, 0.2, data_size)\n",
        "y = (probability > 0.8).astype(int) # Threshold defines pass/fail\n",
        "\n",
        "X = pd.DataFrame({\n",
        "    'study_time': study_time,\n",
        "    'absences': absences,\n",
        "    'internal_scores': internal_scores\n",
        "})\n",
        "\n",
        "# --- Cell 2: Preprocess Data ---\n",
        "\n",
        "# Map labels from {0, 1} to {-1, 1} for SVM\n",
        "y_svm = y.copy()\n",
        "y_svm[y == 0] = -1\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_svm, test_size=0.3, random_state=42, stratify=y_svm)\n",
        "\n",
        "# 1. Scale data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# 2. Apply Polynomial Feature Expansion (degree=2)\n",
        "# This explicitly creates the polynomial features (e.g., a*b, a^2, b^2)\n",
        "# We then run a *linear* SVM on these *new* features.\n",
        "# This is equivalent to using a polynomial kernel in the primal form.\n",
        "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
        "X_train_poly = poly.fit_transform(X_train_scaled)\n",
        "X_test_poly = poly.transform(X_test_scaled)\n",
        "\n",
        "# --- Cell 3: SVM \"from scratch\" Implementation (Sequential) ---\n",
        "# This is the same linear SVM logic as before, but applied to the\n",
        "# polynomial-expanded feature set (X_train_poly).\n",
        "\n",
        "# --- Hyperparameters ---\n",
        "learning_rate = 0.001\n",
        "lambda_param = 0.01 # Regularization parameter\n",
        "n_iters = 1000\n",
        "\n",
        "# --- Initialization ---\n",
        "n_samples, n_features = X_train_poly.shape\n",
        "w = np.zeros(n_features) # Weights\n",
        "b = 0 # Bias\n",
        "\n",
        "# --- Gradient Descent Training Loop ---\n",
        "for _ in range(n_iters):\n",
        "    for idx, x_i in enumerate(X_train_poly):\n",
        "\n",
        "        # Calculate condition: y_i * (w . x_i + b) >= 1\n",
        "        condition = y_train[idx] * (np.dot(x_i, w) + b)\n",
        "\n",
        "        # Hinge Loss Gradient\n",
        "        if condition >= 1:\n",
        "            # Correct margin\n",
        "            dw = 2 * lambda_param * w\n",
        "            db = 0\n",
        "        else:\n",
        "            # Inside margin or wrong side\n",
        "            dw = 2 * lambda_param * w - y_train[idx] * x_i\n",
        "            db = -y_train[idx]\n",
        "\n",
        "        # Update weights and bias\n",
        "        w = w - learning_rate * dw\n",
        "        b = b - learning_rate * db\n",
        "\n",
        "# --- Cell 4: Predict ---\n",
        "\n",
        "# Calculate the linear output on the *polynomial* test features\n",
        "linear_output = np.dot(X_test_poly, w) + b\n",
        "\n",
        "# Predictions are the sign of the output\n",
        "y_pred = np.sign(linear_output)\n",
        "\n",
        "# --- Cell 5: Evaluate Performance ---\n",
        "\n",
        "# Map labels back to {0, 1} for metrics\n",
        "y_test_mapped = (y_test + 1) // 2 # {-1 -> 0, 1 -> 1}\n",
        "y_pred_mapped = (y_pred + 1) // 2 # {-1 -> 0, 1 -> 1}\n",
        "# Handle cases where all preds are -1 (mapped to 0)\n",
        "y_pred_mapped[y_pred_mapped == -1] = 0\n",
        "\n",
        "\n",
        "# --- Cell 6: Display Results ---\n",
        "\n",
        "accuracy = accuracy_score(y_test_mapped, y_pred_mapped)\n",
        "cm = confusion_matrix(y_test_mapped, y_pred_mapped)\n",
        "\n",
        "# Calculate individual metrics for each class\n",
        "precision = precision_score(y_test_mapped, y_pred_mapped, average=None, labels=[0, 1], zero_division=0)\n",
        "recall = recall_score(y_test_mapped, y_pred_mapped, average=None, labels=[0, 1], zero_division=0)\n",
        "f1 = f1_score(y_test_mapped, y_pred_mapped, average=None, labels=[0, 1], zero_division=0)\n",
        "\n",
        "print(\"--- Model Evaluation Results (SVM From Scratch w/ Poly Features) ---\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(\"\\n--- Confusion Matrix ---\")\n",
        "print(cm)\n",
        "print(\"\\n--- Performance Metrics ---\")\n",
        "\n",
        "target_names = ['Fail (0)', 'Pass (1)']\n",
        "print(f\"{'':<10} {'precision':<10} {'recall':<10} {'f1-score':<10}\")\n",
        "print(\"-\" * 40)\n",
        "for i in range(len(target_names)):\n",
        "    print(f\"{target_names[i]:<10} {precision[i]:<10.4f} {recall[i]:<10.4f} {f1[i]:<10.4f}\")\n",
        "\n",
        "# Get metrics for the \"Pass\" class (label 1)\n",
        "precision_pass = precision[1]\n",
        "recall_pass = recall[1]\n",
        "f1_pass = f1[1]\n",
        "\n",
        "print(\"\\n--- Key Metrics (for 'Pass' class) ---\")\n",
        "print(f\"Precision: {precision_pass:.4f}\")\n",
        "print(f\"Recall:    {recall_pass:.4f}\")\n",
        "print(f\"F1-Score:  {f1_pass:.4f}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uJcrhKPIOvAj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ku7IplohOwxH"
      },
      "source": [
        "21\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "6Uz2uE3crip9",
        "outputId": "5ccbf0e6-b864-4464-d34f-bc94413191a0"
      },
      "outputs": [
        {
          "ename": "IndentationError",
          "evalue": "expected an indented block after 'for' statement on line 50 (ipython-input-531363624.py, line 53)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-531363624.py\"\u001b[0;36m, line \u001b[0;32m53\u001b[0m\n\u001b[0;31m    condition = y_train[idx] * (np.dot(x_i, w) + b)\u001b[0m\n\u001b[0m                                                   ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block after 'for' statement on line 50\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, roc_curve, auc\n",
        "\n",
        "# --- Cell 1: Load Data ---\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# --- Cell 2: Preprocess Data ---\n",
        "\n",
        "# Map labels from {0, 1} to {-1, 1} for SVM\n",
        "# Malignant (0) -> -1\n",
        "# Benign (1) -> 1\n",
        "y_svm = y.copy()\n",
        "y_svm[y == 0] = -1\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_svm, test_size=0.3, random_state=42, stratify=y_svm)\n",
        "\n",
        "# 1. Scale data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# 2. Apply Polynomial Feature Expansion (degree=2)\n",
        "# We will run a linear SVM on these new features,\n",
        "# which is equivalent to a polynomial kernel in the primal form.\n",
        "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
        "X_train_poly = poly.fit_transform(X_train_scaled)\n",
        "X_test_poly = poly.transform(X_test_scaled)\n",
        "\n",
        "# --- Cell 3: SVM \"from scratch\" Implementation (Sequential) ---\n",
        "# This is a linear SVM applied to the polynomial-expanded feature set.\n",
        "\n",
        "# --- Hyperparameters ---\n",
        "learning_rate = 0.0001 # Smaller learning rate for stability\n",
        "lambda_param = 0.01 # Regularization parameter\n",
        "n_iters = 1000\n",
        "\n",
        "# --- Initialization ---\n",
        "n_samples, n_features = X_train_poly.shape\n",
        "w = np.zeros(n_features) # Weights\n",
        "b = 0 # Bias\n",
        "\n",
        "# --- Gradient Descent Training Loop ---\n",
        "for _ in range(n_iters):\n",
        "    for idx, x_i in enumerate(X_train_poly):\n",
        "\n",
        "        # Calculate condition: y_i * (w . x_i + b) >= 1\n",
        "        condition = y_train[idx] * (np.dot(x_i, w) + b)\n",
        "\n",
        "        # Hinge Loss Gradient\n",
        "        if condition >= 1:\n",
        "            # Correct margin\n",
        "            dw = 2 * lambda_param * w\n",
        "            db = 0\n",
        "        else:\n",
        "            # Inside margin or wrong side\n",
        "            dw = 2 * lambda_param * w - y_train[idx] * x_i\n",
        "            db = -y_train[idx]\n",
        "\n",
        "        # Update weights and bias\n",
        "        w = w - learning_rate * dw\n",
        "        b = b - learning_rate * db\n",
        "\n",
        "# --- Cell 4: Predict ---\n",
        "\n",
        "# Calculate the linear output (decision scores)\n",
        "# These scores are used for the ROC curve\n",
        "y_scores = np.dot(X_test_poly, w) + b\n",
        "\n",
        "# Predictions are the sign of the output\n",
        "y_pred = np.sign(y_scores)\n",
        "\n",
        "# --- Cell 5: Evaluate Performance ---\n",
        "\n",
        "# Map labels back to {0, 1} for metrics\n",
        "y_test_mapped = (y_test + 1) // 2 # {-1 -> 0, 1 -> 1}\n",
        "y_pred_mapped = (y_pred + 1) // 2 # {-1 -> 0, 1 -> 1}\n",
        "# Handle cases where all preds are -1 (mapped to 0)\n",
        "y_pred_mapped[y_pred_mapped == -1] = 0\n",
        "\n",
        "# --- Confusion Matrix and Accuracy ---\n",
        "accuracy = accuracy_score(y_test_mapped, y_pred_mapped)\n",
        "cm = confusion_matrix(y_test_mapped, y_pred_mapped)\n",
        "\n",
        "print(\"--- Model Evaluation Results (SVM From Scratch w/ Poly Features) ---\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(\"\\n--- Confusion Matrix ---\")\n",
        "print(\"True Neg (Malignant) | False Pos (Benign)\")\n",
        "print(\"False Neg (Malignant)| True Pos (Benign)\")\n",
        "print(cm)\n",
        "\n",
        "# --- ROC Curve and AUC ---\n",
        "# Use the original {0, 1} test labels and the continuous scores\n",
        "fpr, tpr, thresholds = roc_curve(y_test_mapped, y_scores)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "print(f\"\\nAUC (Area Under Curve): {roc_auc:.4f}\")\n",
        "\n",
        "# --- Cell 6: Plot ROC Curve ---\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate (1 - Specificity)')\n",
        "plt.ylabel('True Positive Rate (Sensitivity)')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ysEjHR2rOtsv"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
